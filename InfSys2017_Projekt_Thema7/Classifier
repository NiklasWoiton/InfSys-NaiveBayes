#!/usr/bin/python
# -*- coding: utf-8 -*-

import csv
import nltk
import random


class NBClassifier(object):

    def __init__(self, csvDataSet):
        self.csvDataSet = csvDataSet
        self.listDataSet = []
        self.identifier = []
        self.classifier = None

    def transformedData(self):
        readCSV= csv.reader(open(self.csvDataSet, "rb"))
        self.listDataSet = [tuple(line) for line in readCSV]
        self.identifier = self.listDataSet.pop(0)
        random.shuffle(self.listDataSet)

    def trainClassifier(self, testPartition = 0):
        features_class_tuples = [(self.listDataSet[i], self.listDataSet[i][0]) for i in range(0, len(self.listDataSet))]
        featureSet = [(self.featureTransformation(n), rating) for (n, rating) in features_class_tuples]
        trainSet, testSet = featureSet[int(len(features_class_tuples) * testPartition):], featureSet[:int(len(features_class_tuples) * testPartition)]
        self.classifier = nltk.NaiveBayesClassifier.train(trainSet)
        return testSet

    def testClassifier(self, testPartition):
        if testPartition < 0 or testPartition > 1:
            print "Error: " + str(testPartition) + " is not a valuable Input.\n" + \
                  "Please insert Values between 0 and 1."
        else:
            testSet = self.trainClassifier(testPartition)
            self.classifier.show_most_informative_features(10)
            print "Accuracy: ", nltk.classify.accuracy(self.classifier, testSet)

    def featureTransformation(self,featureTuple):
        features = {}
        for element in range(1, len(featureTuple)):
            features[self.identifier[element]] = featureTuple[element]
        return features

    def classifyTuple(self,tuple):
        print self.classifier.classify(self.featureTransformation(tuple))

    pass

nbClassifier = NBClassifier("workData.csv")
nbClassifier.transformedData()
nbClassifier.testClassifier(0.5)
